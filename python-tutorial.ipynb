{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with python for text processing\n",
    "\n",
    "# Part 0 : Installation\n",
    "The easiest way to get most packages that you need is through installing the anaconda stack.\n",
    "http://continuum.io/downloads\n",
    "\n",
    "You will also need nltk library and optionally textblob for extra functionality:\n",
    "Installing nltk: http://www.nltk.org/install.html\n",
    "Installing nltk data: http://www.nltk.org/data.html \n",
    "[Optional] Installing TextBlob: https://textblob.readthedocs.org/en/dev/\n",
    "\n",
    "___\n",
    "\n",
    "# Part 1: Getting started\n",
    "\n",
    "## Variables\n",
    "\n",
    "Variables can be defined using a custom name and you do not need to explicitly assign types to variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'float'>\n",
      "<type 'str'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "var_str = 'Hello'\n",
    "var_int = 2\n",
    "var_float = 2.0\n",
    "\n",
    "# Variable types:\n",
    "print type(var_float)\n",
    "print type(var_str)\n",
    "print isinstance(var_str, str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['dog', 'cat']\n",
      "[2, 1]\n"
     ]
    }
   ],
   "source": [
    "# Two important types in python are sets and lists\n",
    "mylist = [1,2,5,7]\n",
    "myset = {'a','b'}\n",
    "\n",
    "# dictionaries provide mappings:\n",
    "mydict = {'cat': 1, 'dog': 2}\n",
    "print mydict['cat']\n",
    "print mydict.keys()\n",
    "print mydict.values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Accessing members of a list by index\n",
    "print mylist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 5, 7, 8]\n",
      "set(['a', 'c', 'b'])\n"
     ]
    }
   ],
   "source": [
    "# Adding elements\n",
    "mylist.append(8)\n",
    "print mylist\n",
    "\n",
    "myset.add('c')\n",
    "myset.add('b')\n",
    "print myset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Operations on variables\n",
    "Python supports all standard operations:\n",
    "+, -, *, /, //, %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "11\n",
      "3.66666666667\n",
      "3.66666666667\n",
      "3.66666666667\n",
      "3.66666666667\n",
      "3.0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = 10\n",
    "print a*2 + a**2 \n",
    "\n",
    "a += 1   # i.e: a = a + 1\n",
    "print a\n",
    "\n",
    "\n",
    "# Note: In python 2, ``/`` operator is integer division if inputs are integers.\n",
    "print a/3\n",
    "# If you need float division you can explicitly divide by float\n",
    "print a/3.0\n",
    "print a/float(3)\n",
    "\n",
    "# If you have lots of those operations do the following import on top of your .py file:\n",
    "from __future__ import division\n",
    "print a/3\n",
    "\n",
    "# operator ``//`` explicitly does integer division\n",
    "print a//float(3)\n",
    "\n",
    "# operator ``%`` is the modulo operator\n",
    "print a%3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "\n",
    "## More list and string operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[2, 3, 4]\n",
      "9\n",
      "[6, 7]\n",
      "[0, 1, 2, 3]\n",
      "str1 str2 \n",
      "r1 \n",
      "1\n",
      "-1\n",
      "tr1 \n"
     ]
    }
   ],
   "source": [
    "# access array indices by range\n",
    "a = range(10) # ie. a = [0,1,2,...,9]\n",
    "print a\n",
    "\n",
    "print a[2:5]\n",
    "print a[-1]\n",
    "print a[-4:-2]\n",
    "print a[:4]\n",
    "\n",
    "# Add strings\n",
    "a = 'str1 '\n",
    "b = 'str2 '\n",
    "print a + b\n",
    "print a[2:]\n",
    "print a.find('t')\n",
    "print a.find('G')\n",
    "print a[a.find('t'):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Comprehensions\n",
    "\n",
    "Comprehensions provide a concise way to create lists, sets, dictionaries. Common applications are to make new lists where each element is the result of some operations applied to each member of another sequence or iterable, or to create a subsequence of those elements that satisfy a certain condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "set([1, 4])\n",
      "[('a', 1), ('b', 2), ('c', 3)]\n",
      "{'a': 1, 'c': 3, 'b': 2}\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "for i in range(10):\n",
    "    a.append(i)\n",
    "\n",
    "# List comprehension:\n",
    "b = [i for i in range(10)] # Consie way of defining a list\n",
    "print a == b\n",
    "\n",
    "# Set comprehension:\n",
    "a = {x**2 for x in {-2,-1,1,2}}\n",
    "print a\n",
    "\n",
    "# Dictionary comprehension:\n",
    "keys = ['a', 'b', 'c']\n",
    "values = [1, 2, 3]\n",
    "print zip(keys, values)\n",
    "a = {key: value for key, value in zip(keys, values)}\n",
    "print a\n",
    "print a['a']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Flow control\n",
    "Flow control statements in python are ``if``, ``for``, and ``while``.\n",
    "\n",
    "Python does not provide code block statements such as ``{`` ``}``.\n",
    "Instead code blocks are identified by indents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "evens: [0, 2, 4, 6, 8]\n",
      "odds: [1, 3, 5, 7, 9]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 30, 30]\n"
     ]
    }
   ],
   "source": [
    "a = range(10)\n",
    "\n",
    "for element in a:\n",
    "    if element > 8:\n",
    "        print element\n",
    "\n",
    "evens = []\n",
    "odds = []\n",
    "for element in a:\n",
    "    if element % 2 == 0:\n",
    "        evens.append(element)\n",
    "    elif element % 2 == 1:\n",
    "        odds.append(element)\n",
    "\n",
    "print 'evens: ' + str(evens)\n",
    "print 'odds: ' + str(odds)\n",
    "\n",
    "while len(a) < 12:\n",
    "    a.append(30)\n",
    "\n",
    "print a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Functions\n",
    "\n",
    "In python the ``def`` keyword is used to define functions, short for define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441\n",
      "[5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "    return x**2\n",
    "\n",
    "def first_n_items(x, n):\n",
    "    return x[:n]\n",
    "\n",
    "print square(21)\n",
    "print first_n_items(range(5, 20), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Functions without return statements return None\n",
    "\n",
    "def print_line(a): # Prints a list with each element in a separate line\n",
    "    for element in a:\n",
    "        print element\n",
    "        \n",
    "mylist = range(3)\n",
    "print_line(mylist)\n",
    "print print_line(mylist)\n",
    "\n",
    "# Note the last statement first runs the function and then\n",
    "#    prints it's return value, which in this case is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Classes\n",
    "\n",
    "Python supports Object Oriented Programming principles. The ```class``` statement creates a new class definition. We just give a very basic introduction to class definition in python. For more information please refer to python documentation: https://docs.python.org/2/tutorial/classes.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canine\n",
      "Fido\n",
      "Unknown\n",
      "Labrador\n"
     ]
    }
   ],
   "source": [
    "class Dog:\n",
    "    kind = 'canine'         # class variable shared by all instances\n",
    "\n",
    "    def __init__(self, name, breed=None):\n",
    "        '''\n",
    "        __init__ is a function that is called right after class instantiation\n",
    "        arguments can be mandatory or optional, optional arguments are assigned a default value\n",
    "        '''\n",
    "        self.name = name    # instance variable unique to each instance\n",
    "        if breed is not None:\n",
    "            self.breed = breed\n",
    "        else:\n",
    "            self.breed = 'Unknown'\n",
    "\n",
    "d = Dog('Fido')\n",
    "print d.kind\n",
    "print d.name\n",
    "print d.breed\n",
    "\n",
    "d = Dog('Buddy', 'Labrador')\n",
    "print d.breed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Imports\n",
    "When you want to use a specific package that is not loaded by default, you need to ```import``` it. External libraries are used with the import [libname] keyword. You can also use from [libname] import [funcname] for individual functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import word_tokenize as wrd_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "\n",
    "# Part 1: Preprocessing text\n",
    "\n",
    "Textual data come in different formats. The simplest for is the flat text files.\n",
    "\n",
    "## 1-1 I/O Operations, accessing content of the files\n",
    "\n",
    "Python provides an easy way to perform I/O operations on files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample text to be written to file.\n",
      "This is a sample text to be written to file.\n"
     ]
    }
   ],
   "source": [
    "file_path = '/tmp/test.txt'\n",
    "text = 'This is a sample text to be written to file.'\n",
    "\n",
    "with open(file_path, 'w') as file: # second argument is mode ``w`` is for write\n",
    "    file.write(text)\n",
    "    \n",
    "# Now the file in the ``file_path`` contains the desired text\n",
    "# Let's read it with python:\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    \n",
    "print content\n",
    "\n",
    "# Sometimes you need to handle international characters\n",
    "# In that case you can use the ``codecs`` package\n",
    "\n",
    "import codecs\n",
    "with codecs.open(file_path, 'r', encoding='utf-8') as file:\n",
    "    contents = file.read()\n",
    "\n",
    "print contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2 Preprocessing using python built functions\n",
    "\n",
    "### Tokenization or splitting into words\n",
    "Tokenization is an essential part of any text processing application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albert', 'Einstein', '(14', 'March', '1879', '-', '18', 'April', '1955)', 'was']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"Albert Einstein (14 March 1879 - 18 April 1955) was a German-born theoretical physicist. He developed the general theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics).\n",
    "Einstein's work is also known for its influence on the philosophy of science. Einstein is best known in popular culture for his mass-energy equivalence formula E = mc2 (which has been dubbed \"the world's most famous equation\").\n",
    "He received the 1921 Nobel Prize in Physics for his \"services to theoretical physics\", in particular his discovery of the law of the photoelectric effect, a pivotal step in the evolution of quantum theory.'''\n",
    "\"\"\"\n",
    "a = sentence.split()\n",
    "print a[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein (14 March 1879 - 18 April 1955) was\n"
     ]
    }
   ],
   "source": [
    "# Join lists to form a sentence\n",
    "print ' '.join(a[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the pythons built in split only considers white spaces. In many cases this is not optimal (e.g. in the above sentence ``(14`` is a single token which should not be the case). We will now use nltk to do better preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 1-3 nltk\n",
    "### Sentence tokenization (sentence boundary detection):\n",
    "You can use Punkt Sentence Tokenizer to split a text into sentences.\n",
    "This tokenizer divides a text into a list of sentences, by using an unsupervised algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein (14 March 1879 - 18 April 1955) wa ...\n",
      "He developed the general theory of relativity, one ...\n",
      "Einstein's work is also known for its influence on ...\n",
      "Einstein is best known in popular culture for his  ...\n",
      "He received the 1921 Nobel Prize in Physics for hi ...\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sents = sent_tokenize(sentence)\n",
    "for sent in sents:\n",
    "    print sent[:50] + ' ...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albert', 'Einstein', '(', '14', 'March', '1879', '-', '18', 'April', '1955']\n"
     ]
    }
   ],
   "source": [
    "# The tokenizer that uses the TreeBankTokenizer\n",
    "# Notice that parantices are handledd correctly here\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "print words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albert', 'Einstein', '(', '14', 'March', '1879', '-', '18', 'April', '1955']\n"
     ]
    }
   ],
   "source": [
    "# Another tokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "words = WordPunctTokenizer().tokenize(sentence)\n",
    "print words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Text\n",
    "If often happens that you need to handle terms such as 'Term', 'term' and 'Terms' in a similar way and consider them identical. In order to do that you need to lowercase all the terms and also do stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'albert', u'einstein', u'(', u'14', u'march', u'1879', u'-', u'18', u'april', u'1955']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "porter = nltk.PorterStemmer()\n",
    "words = [porter.stem(w) for w in word_tokenize(sentence.lower())] # Comprehension to stem words\n",
    "print words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[')', u'wa', 'a', 'german-born', 'theoretical', 'physicist', '.', 'he', 'developed', 'the']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization: Changing terms to their lemma\n",
    "import nltk\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "words = [lemmatizer.lemmatize(w) for w in word_tokenize(sentence.lower())]\n",
    "print words[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Part of speech tagging\n",
    "\n",
    "Part of speech (POS) tags are additional information that can help in various text analysis tasks. Nltk provides an easy way to extract POS tags. You will need to download `` taggers/maxent_treebank_pos_tagger/english.pickle`` by nltk.download() to be able to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'NN'), ('sentence', 'NN')]\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = nltk.word_tokenize('This is a sample sentence for which we want to extract part of speech tags.')\n",
    "print nltk.pos_tag(text)[:5]\n",
    "\n",
    "# To get help about specific tags use nltk help module\n",
    "nltk.help.upenn_tagset('VBZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 2- Text classification\n",
    "\n",
    "Text is unstructured and often inorder to find interesting patterns in text, you need to convert it into structured data and extract information from it. For example, if you want to classify text into different categories, you need to change the free form text to features and later use those features for your classifier.\n",
    "Here, we walk you through how to do text classification using python and scikit-learn [1].\n",
    "\n",
    "[1] reference: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: rsilver@world.std.com (Richard Silver)\n",
      "Subject: Barbecued foods and health risk\n",
      "Organization: The World Public Access UNIX, Brookline, MA\n",
      "Lines: 10\n",
      "\n",
      "\n",
      "Some recent postings remind me that I had read about risks \n",
      "associated with the barbecuing of foods, namely that carcinogens \n",
      "are generated. Is this a valid concern? If so, is it a function \n",
      "of the smoke or the elevated temperatures? Is it a function of \n",
      "the cooking elements, wood or charcoal vs. lava rocks? I wish \n",
      "to know more. Thanks. \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "13\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "# First lets get some data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train',\n",
    "    shuffle=True, random_state=2)\n",
    "print train.data[100]\n",
    "print train.target[100]\n",
    "print train.target_names[train.target[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27366\n"
     ]
    }
   ],
   "source": [
    "# We want to assing each text to its corresponding category\n",
    "# We need to extract features\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train.data)\n",
    "print count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Occurrence count has an issue that longer documents often have more words\n",
    "# So we can normalize the occurances by dividing each to total number of words in each document\n",
    "# We also want terms that occur in all documents to have a lower impact, this can be normalized by idf values\n",
    "\n",
    "# Term Frequency features:\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "# Term Frequency - Inverted Document Frequency features:\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85116834838\n"
     ]
    }
   ],
   "source": [
    "# Now we have all features, lets train the classifier and evaluate its performance\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "clf = SGDClassifier().fit(X_train_tfidf, train.target)\n",
    "test = fetch_20newsgroups(subset='test',\n",
    "    shuffle=True, random_state=2)\n",
    "X_test_counts = count_vect.transform(test.data)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "print np.mean(predicted == test.target)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => rec.autos\n"
     ]
    }
   ],
   "source": [
    "# Lets get the categories of some test documents:\n",
    "\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, train.target_names[category]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
